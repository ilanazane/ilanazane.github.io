<!DOCTYPE html>
<html lang="en"><head>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css">
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1"><!-- Begin Jekyll SEO tag v2.8.0 -->
<title>GPT-2: Crafting Culinary Creativity üë©‚Äçüç≥ü•ë | Ilana Zane</title>
<meta name="generator" content="Jekyll v4.3.3" />
<meta property="og:title" content="GPT-2: Crafting Culinary Creativity üë©‚Äçüç≥ü•ë" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="In today‚Äôs world of AI, the possibilities for innovation are endless, even in the kitchen. In this project I aim to leverage GPT-2 Medium to create a recipe generator and explain how to train and finetune this model to create diverse and unique recipes from a simple list of ingredients. GPT-2 Medium, which has 345 million parameters, is a good middle ground between other models while still providing a decent level of sophistication in generating coherent and contextually appropriate text. I found this model to be manageable in terms of the computational resources required for both training and inference." />
<meta property="og:description" content="In today‚Äôs world of AI, the possibilities for innovation are endless, even in the kitchen. In this project I aim to leverage GPT-2 Medium to create a recipe generator and explain how to train and finetune this model to create diverse and unique recipes from a simple list of ingredients. GPT-2 Medium, which has 345 million parameters, is a good middle ground between other models while still providing a decent level of sophistication in generating coherent and contextually appropriate text. I found this model to be manageable in terms of the computational resources required for both training and inference." />
<link rel="canonical" href="http://localhost:4000/projects/2024/08/22/GPT2-Recipe.html" />
<meta property="og:url" content="http://localhost:4000/projects/2024/08/22/GPT2-Recipe.html" />
<meta property="og:site_name" content="Ilana Zane" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2024-08-22T00:00:00-04:00" />
<meta name="twitter:card" content="summary" />
<meta property="twitter:title" content="GPT-2: Crafting Culinary Creativity üë©‚Äçüç≥ü•ë" />
<script type="application/ld+json">
{"@context":"https://schema.org","@type":"BlogPosting","dateModified":"2024-08-22T00:00:00-04:00","datePublished":"2024-08-22T00:00:00-04:00","description":"In today‚Äôs world of AI, the possibilities for innovation are endless, even in the kitchen. In this project I aim to leverage GPT-2 Medium to create a recipe generator and explain how to train and finetune this model to create diverse and unique recipes from a simple list of ingredients. GPT-2 Medium, which has 345 million parameters, is a good middle ground between other models while still providing a decent level of sophistication in generating coherent and contextually appropriate text. I found this model to be manageable in terms of the computational resources required for both training and inference.","headline":"GPT-2: Crafting Culinary Creativity üë©‚Äçüç≥ü•ë","mainEntityOfPage":{"@type":"WebPage","@id":"http://localhost:4000/projects/2024/08/22/GPT2-Recipe.html"},"url":"http://localhost:4000/projects/2024/08/22/GPT2-Recipe.html"}</script>
<!-- End Jekyll SEO tag -->
<link rel="stylesheet" href="/assets/main.css"><link type="application/atom+xml" rel="alternate" href="http://localhost:4000/feed.xml" title="Ilana Zane" /><meta name="google-site-verification" content="-U80l7j-jWyXzli0lOaeIKs8E0K3Rs2hUzeXIwJeoaw" />
  </head>

  <script type="text/x-mathjax-config">
MathJax.Hub.Config({
  tex2jax: {
    inlineMath: [['$','$'], ['\\(','\\)']],
    processEscapes: true
  }
});
</script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script><body><header class="site-header" role="banner">

    <div class="wrapper"><a class="site-title" rel="author" href="/">Ilana Zane</a><nav class="site-nav">
          <input type="checkbox" id="nav-trigger" class="nav-trigger" />
          <label for="nav-trigger">
            <span class="menu-icon">
              <svg viewBox="0 0 18 15" width="18px" height="15px">
                <path d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.032C17.335,0,18,0.665,18,1.484L18,1.484z M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.032C17.335,6.031,18,6.696,18,7.516L18,7.516z M18,13.516C18,14.335,17.335,15,16.516,15H1.484 C0.665,15,0,14.335,0,13.516l0,0c0-0.82,0.665-1.483,1.484-1.483h15.032C17.335,12.031,18,12.695,18,13.516L18,13.516z"/>
              </svg>
            </span>
          </label>
  
          <div class="trigger"><a class="page-link" href="/about/">About</a><a class="page-link" href="/categories/">Categories</a><a class="page-link" href="/research/">Research</a></div>
        </nav><!-- <img src="/assets/images/banner2.png"/> -->
    </div>
  </header>
  <main class="page-content" aria-label="Content">
      <div class="wrapper">
        <article class="post h-entry" itemscope itemtype="http://schema.org/BlogPosting">

  <header class="post-header">
    <h1 class="post-title p-name" itemprop="name headline">GPT-2: Crafting Culinary Creativity üë©‚Äçüç≥ü•ë </h1>
    <p class="post-meta">
      <time class="dt-published" datetime="2024-08-22T00:00:00-04:00" itemprop="datePublished">Aug 22, 2024
      </time></p>
  </header>

  <div class="post-content e-content" itemprop="articleBody">
    <p>In today‚Äôs world of AI, the possibilities for innovation are endless, even in the kitchen. In this project I aim to leverage GPT-2 Medium to create a recipe generator and explain how to train and finetune this model to create diverse and unique recipes from a simple list of ingredients. 
GPT-2 Medium, which has 345 million parameters, is a good middle ground between other models while still providing a decent level of sophistication in generating coherent and contextually appropriate text. I found this model to be manageable in terms of the computational resources required for both training and inference.</p>

<p>More information about this <a href="https://huggingface.co/openai-community/gpt2-medium" target="_blank"> model </a> can be found in the model card from Hugging Face</p>

<h1 id="text-preprocessing">Text Preprocessing</h1>
<p>The data that was used for this project came from Kaggle. The <a href="https://www.kaggle.com/datasets/thedevastator/better-recipes-for-a-better-life" target="_blank"> dataset </a></p>

<p>the data set was reduced to include the following important features:</p>
<ul>
  <li>recipe_name: The name of the recipe. (String)</li>
  <li>ingredients: A list of ingredients required to make the recipe. (List)</li>
  <li>directions: A list of directions for preparing and cooking the recipe. (List)</li>
</ul>

<h1 id="training-and-finetuning">Training and Finetuning</h1>
<p>The first step to training the model is to convert the csv file into sequential text. This is important because language models like GPT-2 are trained to predict the next word in a sequence, given the previous words. For the model to understand how ingredients relate to the recipe name and directs, the data needs to show examples of how these elements are sequenced together in natural language i.e. as a continuous flow of text, much like how a human would read and understand a recipe. These models are designed to work with plain text data. While a CSV file is structured, it needs to be converted into a format that the model can understand.</p>

<p>It is possible to train the model with high-performance CPU‚Äôs, but training was much more manageable with access to a GPU.</p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># check if GPU is available
</span><span class="n">device</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">device</span><span class="p">(</span><span class="sh">"</span><span class="s">cuda</span><span class="sh">"</span> <span class="k">if</span> <span class="n">torch</span><span class="p">.</span><span class="n">cuda</span><span class="p">.</span><span class="nf">is_available</span><span class="p">()</span> <span class="k">else</span> <span class="sh">"</span><span class="s">cpu</span><span class="sh">"</span><span class="p">)</span>
</code></pre></div></div>
<p>The model and tokenizer are loaded using the <a href="https://huggingface.co/docs/transformers/en/index" target="_blank">transformers </a> library from Hugging Face</p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">model_name</span> <span class="o">=</span> <span class="sh">"</span><span class="s">gpt2-medium</span><span class="sh">"</span>
<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">GPT2Tokenizer</span><span class="p">.</span><span class="nf">from_pretrained</span><span class="p">(</span><span class="n">model_name</span><span class="p">)</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">GPT2LMHeadModel</span><span class="p">.</span><span class="nf">from_pretrained</span><span class="p">(</span><span class="n">model_name</span><span class="p">)</span>
<span class="n">model</span><span class="p">.</span><span class="nf">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
</code></pre></div></div>

<p>The tokenizer is responsible for converting raw text into tokens(numbers) that the model can understand.</p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">tokenizer</span> <span class="o">=</span> <span class="n">GPT2Tokenizer</span><span class="p">.</span><span class="nf">from_pretrained</span><span class="p">(</span><span class="n">model_name</span><span class="p">)</span>
<span class="n">tokenizer</span><span class="p">.</span><span class="n">add_special_tokens</span><span class="p">{</span><span class="sh">"</span><span class="s">pad_token</span><span class="sh">"</span><span class="p">:</span> <span class="sh">"</span><span class="s">[PAD]</span><span class="sh">"</span><span class="p">}</span>
</code></pre></div></div>
<p>Padding tokens are used to ensure that all sequences in a batch are of the same length by filling shorter sequences with the padding token. While not required by GPT2-Medium, having uniform sequence lengths across a batch during training allows for efficient computation as it simplifies the implementation of parallel processing on GPUs.
It is important to then resize the model‚Äôs token embeddings to match the new size of the tokenizer‚Äôs vocabulary. Since the vocabulary size increased due to the addition of the padding token, the model‚Äôs embeddings are resized to include the new token.</p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">model</span><span class="p">.</span><span class="nf">resize_token_embeddings</span><span class="p">(</span><span class="nf">len</span><span class="p">(</span><span class="n">tokenizer</span><span class="p">))</span>

</code></pre></div></div>
<p>The data collator is designed to handle the preparation of batches of data. This includes managing padding, masking, and other preprocessing tasks that are required when feeding data into a language model. 
One of the parameters included is masked language modeling (MLM), which is set to false, therefore indicating that we are using a standard language modeling setup. This parameter specifies whether the collator should create masked tokens (for models like BERT) or not. For GPT-2, this should be set to false because it is not trained using MLM but rather by predicting the next token in a sequence.</p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">data_collator</span> <span class="o">=</span> <span class="nc">DataCollatorForLanguageModeling</span><span class="p">(</span><span class="n">tokenizer</span><span class="o">=</span><span class="n">tokenizer</span><span class="p">,</span> <span class="n">mlm</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
</code></pre></div></div>

<p>After playing around with some of the training hyperparameters are I settled on these:</p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># set the training arguments 
</span><span class="n">training_args</span> <span class="o">=</span> <span class="nc">TrainingArguments</span><span class="p">(</span>
    <span class="n">output_dir</span><span class="o">=</span><span class="sh">"</span><span class="s">./results</span><span class="sh">"</span><span class="p">,</span>
    <span class="n">overwrite_output_dir</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span>
    <span class="n">num_train_epochs</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span>
    <span class="n">per_device_train_batch_size</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span>
    <span class="n">save_steps</span><span class="o">=</span><span class="mi">10000</span><span class="p">,</span>
    <span class="n">save_total_limit</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span>
<span class="p">)</span>
</code></pre></div></div>

<h1 id="generating-outputs">Generating Outputs</h1>
<p>A simple Flask app was created to read in ingredients from a user, which were then encoded using the tokenizer. 
The attention mask is used by the model to differentiate between real tokens and padding tokens. During training or inference, the model should only pay attention to the real tokens and ignore the padding tokens.</p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">attention_mask</span> <span class="o">=</span> <span class="n">inputs</span><span class="p">.</span><span class="nf">ne</span><span class="p">(</span><span class="n">tokenizer</span><span class="p">.</span><span class="n">pad_token_id</span><span class="p">).</span><span class="nf">long</span><span class="p">()</span>
</code></pre></div></div>
<p>The following hyperparameters were used when generating the recipes:</p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">outputs</span> <span class="o">=</span> <span class="n">model</span><span class="p">.</span><span class="nf">generate</span><span class="p">(</span>
        <span class="n">inputs</span><span class="p">,</span>
        <span class="n">no_repeat_ngram_size</span> <span class="o">=</span> <span class="mi">2</span><span class="p">,</span>
        <span class="n">attention_mask</span><span class="o">=</span><span class="n">attention_mask</span><span class="p">,</span>
        <span class="n">max_length</span><span class="o">=</span><span class="mi">250</span><span class="p">,</span>
        <span class="n">num_return_sequences</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
        <span class="n">temperature</span> <span class="o">=</span> <span class="mf">0.7</span><span class="p">,</span> <span class="c1"># adjusts randomness
</span>    <span class="p">)</span>
</code></pre></div></div>
<p><em>no_repeat_ngram_size prevents</em> the model from repeating any n-grams of a certain size. In this case, setting the hyperparameter to 2 ensures that no sequence of 2 tokens will be repeated in the generated text which helps to avoid repetetive outputs.</p>

<p><em>max_length</em> specifies the maximum length of the generated sequenece.</p>

<p><em>num_return_sequences</em> specifies the number of generated sequences to return</p>

<p><em>temperature</em> controls the randomness of the text generation. A lower temperature makes the output more focused and deterministic, while a higher temperature makes the output more random and diverse.</p>

<p>The outputs are then decoded with the tokenizer and sent back through the Flask app.</p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">recipe</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">.</span><span class="nf">decode</span><span class="p">(</span><span class="n">outputs</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">skip_special_tokens</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
</code></pre></div></div>

<p>Here is an example of one of the results:</p>

<p><img src="http://localhost:4000/assets/images/GPT2_files/GPT_output.png" alt="image" height="400" width="900" /></p>

<p>The maraschino cherries are just an extra lil treat üòöü§å</p>

<p><a href="https://github.com/ilanazane/Recipe-Generation-GPT-2/tree/main" target="_blank"> Full code </a> can be found on my github.</p>


  </div><a class="u-url" href="/projects/2024/08/22/GPT2-Recipe.html" hidden></a>
</article>

      </div>
    </main><footer class="site-footer h-card">
  <data class="u-url" href="/"></data>

  <div class="wrapper">

    <h2 class="footer-heading">Ilana Zane</h2>

    <div class="footer-col-wrapper">
      <div class="footer-col footer-col-1">
        <ul class="contact-list">
          <li class="p-name">Ilana Zane</li><li><a class="u-email" href="mailto:ilanazane@comcast.net">ilanazane@comcast.net</a></li></ul>
      </div>

      <div class="footer-col footer-col-2"><ul class="social-media-list"><li><a href="https://github.com/ilanazane"><svg class="svg-icon"><use xlink:href="/assets/minima-social-icons.svg#github"></use></svg> <span class="username">ilanazane</span></a></li><li><a href="https://instagram.com/lanadelzane"><svg class="svg-icon"><use xlink:href="/assets/minima-social-icons.svg#instagram"></use></svg> <span class="username">lanadelzane</span></a></li></ul>
</div>

      <div class="footer-col footer-col-3">
        <p>A blog about AI and other interests </p>
      </div>
    </div>

  </div>

</footer>
</body>

</html>
